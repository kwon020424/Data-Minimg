{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwon020424/Data-Minimg/blob/main/%EB%B2%A1%ED%84%B0%EC%99%80_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ë²¡í„°ë¥¼ í™œìš©í•œ ìì—°ì–´ì²˜ë¦¬ ê³¼ì • ì‹¤ìŠµí•˜ê¸°"
      ],
      "metadata": {
        "id": "n3Ho_FnGZa5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "NeagkubncNPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ì£¼ì œ 1: ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ê¸° (Word Embedding)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“š ì£¼ì œ 1: ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ê¸°\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ì˜ˆì œ 1-1: ê°„ë‹¨í•œ ë‹¨ì–´ ë²¡í„° ë§Œë“¤ê¸°\n",
        "print(\"\\n[ì‹¤ìŠµ 1-1] í•™êµìƒí™œ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ê¸°\")\n",
        "\n",
        "# ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„ [í•™ì—…ì ìˆ˜, ì¹œêµ¬ê´€ê³„ì ìˆ˜] 1~10, 1~10\n",
        "words = {\n",
        "    'Math': [10, 3],\n",
        "    'Korea': [9, 4],\n",
        "    'Sport': [3, 10],\n",
        "    'Music': [5, 8],\n",
        "    'Lunch': [2, 9],\n",
        "    'Test': [10, 2]\n",
        "}\n",
        "\n",
        "print(\"\\në‹¨ì–´ ë²¡í„°:\")\n",
        "for word, vector in words.items():\n",
        "    print(f\"  {word}: {vector}\")\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(8, 6))\n",
        "for word, vector in words.items():\n",
        "    x, y = vector\n",
        "    plt.scatter(x, y, s=200, alpha=0.6)\n",
        "    plt.annotate(word, (x, y), fontsize=12, ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel('Study', fontsize=12)\n",
        "plt.ylabel('Friend/Social', fontsize=12)\n",
        "plt.title('2-dim Vectorize', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(0, 12)\n",
        "plt.ylim(0, 12)\n",
        "plt.axhline(y=0, color='k', linewidth=0.5)\n",
        "plt.axvline(x=0, color='k', linewidth=0.5)\n",
        "\n",
        "print(\"\\nğŸ“ ê´€ì°°:\")\n",
        "print(\"  - 'ìˆ˜í•™'ê³¼ 'ì‹œí—˜'ì€ í•™ì—… ìª½ì— ê°€ê¹Œì´ ìˆì–´ìš”\")\n",
        "print(\"  - 'ì²´ìœ¡'ê³¼ 'ì ì‹¬'ì€ ì¹œêµ¬ê´€ê³„ ìª½ì— ê°€ê¹Œì´ ìˆì–´ìš”\")\n",
        "print(\"  - ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ë“¤ì´ ê°€ê¹Œì´ ëª¨ì—¬ìˆì£ !\")"
      ],
      "metadata": {
        "id": "A2gjQGw_cTPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ì£¼ì œ 2: ë²¡í„° ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚° (ìœ í´ë¦¬ë“œ ê±°ë¦¬)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ ì£¼ì œ 2: ë‹¨ì–´ ì‚¬ì´ì˜ ê±°ë¦¬ ì¬ê¸°\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ì˜ˆì œ 2-1: ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°\n",
        "print(\"\\n[ì‹¤ìŠµ 2-1] ë‹¨ì–´ ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚°í•˜ê¸°\")\n",
        "##ê³¼ì œ1.\n",
        "def euclidean_distance(vec1, vec2):\n",
        "    \"\"\"ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚° (ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬)\"\"\"\n",
        "    vec1 = np.array(vec1)\n",
        "    vec2 = np.array(vec2)\n",
        "    return np.sqrt(np.sum((vec1 - vec2)**2))\n",
        "\n",
        "# ê±°ë¦¬ ê³„ì‚° ì˜ˆì‹œ\n",
        "print(\"\\n'ìˆ˜í•™'ê³¼ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ê±°ë¦¬:\")\n",
        "math_vector = words['Math']\n",
        "\n",
        "distances = {}\n",
        "for word, vector in words.items():\n",
        "    if word != 'Math':\n",
        "        dist = euclidean_distance(math_vector, vector)\n",
        "        distances[word] = dist\n",
        "        print(f\"  ìˆ˜í•™ â†” {word}: {dist:.2f}\")\n",
        "\n",
        "# ê°€ì¥ ê°€ê¹Œìš´ ë‹¨ì–´ ì°¾ê¸°\n",
        "closest = min(distances, key=distances.get)\n",
        "farthest = max(distances, key=distances.get)\n",
        "\n",
        "print(f\"\\nâœ… 'ìˆ˜í•™'ê³¼ ê°€ì¥ ë¹„ìŠ·í•œ ë‹¨ì–´: {closest} (ê±°ë¦¬: {distances[closest]:.2f})\")\n",
        "print(f\"âŒ 'ìˆ˜í•™'ê³¼ ê°€ì¥ ë‹¤ë¥¸ ë‹¨ì–´: {farthest} (ê±°ë¦¬: {distances[farthest]:.2f})\")\n",
        "\n",
        "# ì˜ˆì œ 2-2: ê±°ë¦¬ ì‹œê°í™”\n",
        "print(\"\\n[ì‹¤ìŠµ 2-2] ê±°ë¦¬ ì‹œê°í™”\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# ëª¨ë“  ë‹¨ì–´ í‘œì‹œ\n",
        "for word, vector in words.items():\n",
        "    x, y = vector\n",
        "    plt.scatter(x, y, s=200, alpha=0.6)\n",
        "    plt.annotate(word, (x, y), fontsize=12, ha='center', va='bottom')\n",
        "\n",
        "# ìˆ˜í•™ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´/ë¨¼ ë‹¨ì–´ë¡œ ì„  ê·¸ë¦¬ê¸°\n",
        "math_x, math_y = words['Math']\n",
        "closest_x, closest_y = words[closest]\n",
        "farthest_x, farthest_y = words[farthest]\n",
        "\n",
        "plt.plot([math_x, closest_x], [math_y, closest_y],\n",
        "         'g--', linewidth=2, label=f'Close: {distances[closest]:.2f}')\n",
        "plt.plot([math_x, farthest_x], [math_y, farthest_y],\n",
        "         'r--', linewidth=2, label=f'Distance: {distances[farthest]:.2f}')\n",
        "\n",
        "plt.xlabel('Study', fontsize=12)\n",
        "plt.ylabel('Friend/Social Skill', fontsize=12)\n",
        "plt.title('Ddistance(Uclidean)', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(0, 12)\n",
        "plt.ylim(0, 12)\n"
      ],
      "metadata": {
        "id": "JNe_URv8cVL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ì£¼ì œ 3: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ê°ë„ë¡œ ìœ ì‚¬ì„± ì¸¡ì •)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ ì£¼ì œ 3: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ - ê°ë„ë¡œ ë¹„êµí•˜ê¸°\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ì˜ˆì œ 3-1: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "print(\"\\n[ì‹¤ìŠµ 3-1] ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°í•˜ê¸°\")\n",
        "## ê³¼ì œ2.\n",
        "def cosine_sim(vec1, vec2):\n",
        "    \"\"\"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
        "    vec1 = np.array(vec1)\n",
        "    vec2 = np.array(vec2)\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "similarities = {}\n",
        "for word, vector in words.items():\n",
        "    if word != 'Math':\n",
        "        sim = cosine_sim(math_vector, vector)\n",
        "        similarities[word] = sim\n",
        "        # ê°ë„ ê³„ì‚°\n",
        "        angle = np.arccos(sim) * 180 / np.pi\n",
        "        print(f\"  Math â†” {word}: {sim:.3f} (Angle: {angle:.1f}Â°)\")\n",
        "\n",
        "most_similar = max(similarities, key=similarities.get)\n",
        "least_similar = min(similarities, key=similarities.get)\n",
        "\n",
        "print(f\"\\nâœ… 'ìˆ˜í•™'ê³¼ ê°€ì¥ ìœ ì‚¬: {most_similar} ({similarities[most_similar]:.3f})\")\n",
        "print(f\"âŒ 'ìˆ˜í•™'ê³¼ ê°€ì¥ ë‹¤ë¦„: {least_similar} ({similarities[least_similar]:.3f})\")\n",
        "\n",
        "# ì˜ˆì œ 3-2: ê±°ë¦¬ vs ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ\n",
        "print(\"\\n[ì‹¤ìŠµ 3-2] í¬ê¸°ê°€ ë‹¤ë¥¸ ë²¡í„° ë¹„êµ\")\n",
        "\n",
        "# ê°™ì€ ë°©í–¥, ë‹¤ë¥¸ í¬ê¸°ì˜ ë²¡í„°\n",
        "vec_small = [2, 1]   # \"ì¢‹ë‹¤\"\n",
        "vec_large = [8, 4]   # \"ë§¤ìš° ì¢‹ë‹¤\"\n",
        "\n",
        "dist = euclidean_distance(vec_small, vec_large)\n",
        "similarity = cosine_sim(vec_small, vec_large)\n",
        "\n",
        "print(f\"\\n[2, 1]ê³¼ [8, 4] ë¹„êµ:\")\n",
        "print(f\"  ìœ í´ë¦¬ë“œ ê±°ë¦¬: {dist:.2f} (ë©€ë‹¤ê³  íŒë‹¨)\")\n",
        "print(f\"  ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {similarity:.3f} (ì™„ì „íˆ ê°™ì€ ë°©í–¥!)\")\n",
        "print(f\"\\nğŸ’¡ í•´ì„: í¬ê¸°ëŠ” ë‹¤ë¥´ì§€ë§Œ 'ë°©í–¥(ì˜ë¯¸)'ì€ ì™„ì „íˆ ê°™ì•„ìš”!\")"
      ],
      "metadata": {
        "id": "IokpFXizcckr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ì£¼ì œ 4: ë‹¨ì–´ ë²¡í„° ì—°ì‚° (ë²¡í„° ë§ì…ˆê³¼ ëº„ì…ˆ)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"â• ì£¼ì œ 4: ë‹¨ì–´ ë²¡í„° ì—°ì‚° - ì˜ë¯¸ì˜ ìˆ˜í•™!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ì˜ˆì œ 4-1: ë²¡í„° ì—°ì‚°\n",
        "print(\"\\n[ì‹¤ìŠµ 4-1] ë‹¨ì–´ ë²¡í„° ì—°ì‚°í•˜ê¸°\")\n",
        "\n",
        "# ìƒˆë¡œìš´ ë‹¨ì–´ ë²¡í„° ë§Œë“¤ê¸°\n",
        "words['Math'] = [10, 3]\n",
        "words['Test'] = [10, 2]\n",
        "words['Sport'] = [3, 10]\n",
        "words['Friend'] = [0, 5]\n",
        "\n",
        "print(\"\\nì›ë˜ ë‹¨ì–´ë“¤:\")\n",
        "print(f\"  ìˆ˜í•™: {words['Math']}\")\n",
        "print(f\"  ì‹œí—˜: {words['Test']}\")\n",
        "print(f\"  ì²´ìœ¡: {words['Sport']}\")\n",
        "\n",
        "\n",
        "# ìˆ˜í•™ + ì²´ìœ¡ = ?\n",
        "result = np.array(words['Math']) + np.array(words['Sport'])\n",
        "print(f\"\\nìˆ˜í•™ + ì²´ìœ¡ = {result.tolist()}\")\n",
        "#print(\"  â†’ í•™ì—…ë„ í•˜ê³  ìš´ë™ë„ í•˜ëŠ”... 'ì²´ìœ¡ëŒ€íšŒ'?\")\n",
        "\n",
        "# ì²´ìœ¡ - ì¹œêµ¬ê´€ê³„ = ?\n",
        "# ì²´ìœ¡ [3, 10]ì—ì„œ ì¹œêµ¬ê´€ê³„ë¥¼ ì¤„ì´ë©´\n",
        "result2 = np.array(words['Sport']) - np.array([0, 5])\n",
        "print(f\"\\nì²´ìœ¡ - [ì¹œêµ¬ê´€ê³„ ë§ì´] = {result2.tolist()}\")\n",
        "#print(\"  â†’ í˜¼ì í•˜ëŠ” ìš´ë™, 'í—¬ìŠ¤'?\")\n",
        "\n",
        "# ì˜ˆì œ 4-2: ë‹¨ì–´ ìœ ì¶” (Word Analogy)\n",
        "print(\"\\n[ì‹¤ìŠµ 4-2] ë‹¨ì–´ ìœ ì¶” ê²Œì„\")\n",
        "print(\"\\në¬¸ì œ: 'ìˆ˜í•™'ì€ 'ì‹œí—˜'ê³¼ ê´€ê³„ê°€ ìˆë“¯ì´, 'ì²´ìœ¡'ì€ ë¬´ì—‡ê³¼ ê´€ê³„ê°€ ìˆì„ê¹Œ?\")\n",
        "print(\"ì‹: ìˆ˜í•™ - ì‹œí—˜ + ì²´ìœ¡ = ?\")\n",
        "\n",
        "vec_math = np.array(words['Math'])\n",
        "vec_test = np.array(words['Test'])\n",
        "vec_pe = np.array(words['Sport'])\n",
        "\n",
        "result_vec = vec_math - vec_test + vec_pe\n",
        "print(f\"\\nê³„ì‚° ê²°ê³¼ ë²¡í„°: {result_vec}\")\n",
        "\n",
        "# ê°€ì¥ ê°€ê¹Œìš´ ë‹¨ì–´ ì°¾ê¸°\n",
        "print(\"\\nê° ë‹¨ì–´ì™€ì˜ ê±°ë¦¬:\")\n",
        "min_dist = float('inf')\n",
        "answer = None\n",
        "\n",
        "for word, vector in words.items():\n",
        "    if word not in ['Math', 'Test', 'Sport']:\n",
        "        dist = euclidean_distance(result_vec, vector)\n",
        "        print(f\"  {word}: {dist:.2f}\")\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            answer = word\n",
        "\n",
        "print(f\"\\nâœ… ì •ë‹µ: {answer}!\")"
      ],
      "metadata": {
        "id": "CmgsdzYrceLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ì£¼ì œ 5: ë¬¸ì¥ ë²¡í„°í™” (Bag of Words)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ ì£¼ì œ 5: ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë§Œë“¤ê¸°\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "ğŸ’¡ ê°œë…:\n",
        "ë¬¸ì¥ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‘œí˜„í•  ìˆ˜ ìˆì–´ìš”!\n",
        "ë‹¨ì–´ ì£¼ë¨¸ë‹ˆ(Bag of Words) ë°©ë²•: ê° ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì„¸ê¸°\n",
        "\n",
        "ì˜ˆì‹œ ë¬¸ì¥: \"ë‚˜ëŠ” ìˆ˜í•™ì„ ì¢‹ì•„í•˜ê³  ê³¼í•™ë„ ì¢‹ì•„í•œë‹¤\"\n",
        "ë‹¨ì–´ ìˆœì„œ: [ë‚˜, ëŠ”, ìˆ˜í•™, ì„, ì¢‹ì•„í•˜, ê³ , ê³¼í•™, ë„, í•œë‹¤]\n",
        "ë²¡í„°: ê° ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜\n",
        "\"\"\")\n",
        "\n",
        "# ì˜ˆì œ 5-1: ê°„ë‹¨í•œ Bag of Words\n",
        "print(\"\\n[ì‹¤ìŠµ 5-1] ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ê¸°\")\n",
        "\n",
        "# ì„¸ ê°œì˜ í•™êµ ê´€ë ¨ ë¬¸ì¥\n",
        "sentences = [\n",
        "    \"ìˆ˜í•™ ì‹œí—˜ ê³µë¶€\",\n",
        "    \"ì²´ìœ¡ ì‹œê°„ ì¹œêµ¬\",\n",
        "    \"ìˆ˜í•™ ì²´ìœ¡ ê³µë¶€\"\n",
        "]\n",
        "\n",
        "# ì „ì²´ ë‹¨ì–´ ì‚¬ì „ ë§Œë“¤ê¸°\n",
        "all_words = []\n",
        "for sentence in sentences:\n",
        "    all_words.extend(sentence.split())\n",
        "vocabulary = sorted(set(all_words))\n",
        "\n",
        "print(f\"\\në‹¨ì–´ ì‚¬ì „: {vocabulary}\")\n",
        "print(f\"(ì´ {len(vocabulary)}ê°œ ë‹¨ì–´)\")\n",
        "##ê³¼ì œ3.\n",
        "# ê° ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜\n",
        "print(\"\\nê° ë¬¸ì¥ì˜ ë²¡í„° í‘œí˜„:\")\n",
        "sentence_vectors = []\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    words_in_sentence = sentence.split()\n",
        "    vector = [words_in_sentence.count(word) for word in vocabulary]\n",
        "    sentence_vectors.append(vector)\n",
        "    print(f\"\\në¬¸ì¥ {i+1}: '{sentence}'\")\n",
        "    print(f\"  ë²¡í„°: {vector}\")\n",
        "    print(f\"  ì˜ë¯¸: \", end=\"\")\n",
        "    for j, word in enumerate(vocabulary):\n",
        "        if vector[j] > 0:\n",
        "            print(f\"{word}({vector[j]}ë²ˆ) \", end=\"\")\n",
        "    print()\n",
        "\n",
        "# ì˜ˆì œ 5-2: ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ë¹„êµ\n",
        "print(\"\\n[ì‹¤ìŠµ 5-2] ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ê³„ì‚°\")\n",
        "\n",
        "print(\"\\në¬¸ì¥ 1ê³¼ ë‹¤ë¥¸ ë¬¸ì¥ë“¤ì˜ ìœ ì‚¬ë„:\")\n",
        "for i in range(1, len(sentences)):\n",
        "    similarity = cosine_sim(sentence_vectors[0], sentence_vectors[i])\n",
        "    print(f\"  ë¬¸ì¥ 1 â†” ë¬¸ì¥ {i+1}: {similarity:.3f}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ í•´ì„:\")\n",
        "print(\"  - 'ìˆ˜í•™'ì´ë¼ëŠ” ê³µí†µ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ìœ ì‚¬ë„ê°€ ë†’ì•„ìš”\")\n",
        "print(\"  - ê°™ì€ ë‹¨ì–´ê°€ ë§ì„ìˆ˜ë¡ ë²¡í„°ê°€ ë¹„ìŠ·í•´ì ¸ìš”\")"
      ],
      "metadata": {
        "id": "B4yvttN7ckA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ì£¼ì œ 6: TF-IDF (ì¤‘ìš”í•œ ë‹¨ì–´ ì°¾ê¸°)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"â­ ì£¼ì œ 6: ì¤‘ìš”í•œ ë‹¨ì–´ ì°¾ê¸° (TF-IDF)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "ğŸ’¡ ê°œë…:\n",
        "ëª¨ë“  ë‹¨ì–´ê°€ ë˜‘ê°™ì´ ì¤‘ìš”í•œ ê±´ ì•„ë‹ˆì—ìš”!\n",
        "\n",
        "TF (Term Frequency): ë¬¸ì„œì—ì„œ ë‹¨ì–´ê°€ ë‚˜ì˜¨ íšŸìˆ˜\n",
        "IDF (Inverse Document Frequency): í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ì¤‘ìš”\n",
        "\n",
        "ì˜ˆì‹œ:\n",
        "- \"ì´\", \"ê·¸\", \"ì €\" â†’ ìì£¼ ë‚˜ì˜¤ì§€ë§Œ ì¤‘ìš”í•˜ì§€ ì•ŠìŒ (ë‚®ì€ ì ìˆ˜)\n",
        "- \"ì–‘ìì—­í•™\", \"ë¯¸ì ë¶„\" â†’ ì ê²Œ ë‚˜ì˜¤ì§€ë§Œ ì¤‘ìš”í•¨ (ë†’ì€ ì ìˆ˜)\n",
        "\n",
        "TF-IDF = TF Ã— IDF\n",
        "\"\"\")\n",
        "\n",
        "# ì˜ˆì œ 6-1: TF-IDF ê³„ì‚°\n",
        "print(\"\\n[ì‹¤ìŠµ 6-1] TF-IDFë¡œ ì¤‘ìš”í•œ ë‹¨ì–´ ì°¾ê¸°\")\n",
        "\n",
        "# í•™êµ ê´€ë ¨ ë¬¸ì„œë“¤\n",
        "documents = [\n",
        "    \"ìˆ˜í•™ì€ ë…¼ë¦¬ì  ì‚¬ê³ ë¥¼ í‚¤ì›Œìš”\",\n",
        "    \"ì²´ìœ¡ì€ ê±´ê°•ê³¼ ì²´ë ¥ì„ í‚¤ì›Œìš”\",\n",
        "    \"ìŒì•…ì€ ê°ì„±ê³¼ ì°½ì˜ë ¥ì„ í‚¤ì›Œìš”\",\n",
        "    \"ìˆ˜í•™ê³¼ ê³¼í•™ì€ ë…¼ë¦¬ì  ì‚¬ê³ ê°€ ì¤‘ìš”í•´ìš”\"\n",
        "]\n",
        "\n",
        "print(\"\\në¬¸ì„œë“¤:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"  ë¬¸ì„œ {i+1}: {doc}\")\n",
        "\n",
        "# ê°„ë‹¨í•œ TF-IDF ê³„ì‚° (ì‹¤ì œë¡œëŠ” sklearn ì‚¬ìš©)\n",
        "def simple_tfidf(documents):\n",
        "    \"\"\"ê°„ë‹¨í•œ TF-IDF ê³„ì‚°\"\"\"\n",
        "    # 1. ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘\n",
        "    all_words = set()\n",
        "    for doc in documents:\n",
        "        words = doc.replace('ì€', '').replace('ì„', '').replace('ì™€', '').replace('ê°€', '').split()\n",
        "        all_words.update(words)\n",
        "\n",
        "    # 2. TF ê³„ì‚° (ë¬¸ì„œì—ì„œ ë‹¨ì–´ ë¹ˆë„)\n",
        "    tf_scores = []\n",
        "    for doc in documents:\n",
        "        words = doc.replace('ì€', '').replace('ì„', '').replace('ì™€', '').replace('ê°€', '').split()\n",
        "        tf = {}\n",
        "        for word in all_words:\n",
        "            tf[word] = words.count(word)\n",
        "        tf_scores.append(tf)\n",
        "\n",
        "    # 3. IDF ê³„ì‚° (ì „ì²´ ë¬¸ì„œì—ì„œ ë“±ì¥ íšŸìˆ˜ì˜ ì—­ìˆ˜)\n",
        "    idf_scores = {}\n",
        "    num_docs = len(documents)\n",
        "    for word in all_words:\n",
        "        doc_count = sum(1 for doc in documents if word in doc)\n",
        "        idf_scores[word] = np.log(num_docs / (1 + doc_count))\n",
        "\n",
        "    # 4. TF-IDF ê³„ì‚°\n",
        "    tfidf_scores = []\n",
        "    for tf in tf_scores:\n",
        "        tfidf = {}\n",
        "        for word in all_words:\n",
        "            tfidf[word] = tf[word] * idf_scores[word]\n",
        "        tfidf_scores.append(tfidf)\n",
        "\n",
        "    return tfidf_scores, idf_scores\n",
        "\n",
        "tfidf_scores, idf_scores = simple_tfidf(documents)\n",
        "\n",
        "# ê° ë¬¸ì„œì˜ ì¤‘ìš” ë‹¨ì–´ ì¶œë ¥\n",
        "print(\"\\nê° ë¬¸ì„œì˜ ì¤‘ìš” ë‹¨ì–´ (TF-IDF ì ìˆ˜):\")\n",
        "for i, tfidf in enumerate(tfidf_scores):\n",
        "    print(f\"\\në¬¸ì„œ {i+1}: {documents[i]}\")\n",
        "    # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
        "    sorted_words = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"  ì¤‘ìš” ë‹¨ì–´:\")\n",
        "    for word, score in sorted_words[:3]:  # ìƒìœ„ 3ê°œë§Œ\n",
        "        if score > 0:\n",
        "            print(f\"    {word}: {score:.3f}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ í•´ì„:\")\n",
        "print(\"  - 'í‚¤ì›Œìš”'ëŠ” ëª¨ë“  ë¬¸ì„œì— ë‚˜ì™€ì„œ ì ìˆ˜ê°€ ë‚®ì•„ìš” (ì¼ë°˜ì )\")\n",
        "print(\"  - 'ìˆ˜í•™', 'ì²´ìœ¡', 'ìŒì•…'ì€ íŠ¹ì • ë¬¸ì„œì—ë§Œ ë‚˜ì™€ì„œ ì ìˆ˜ê°€ ë†’ì•„ìš” (íŠ¹ë³„í•¨)\")"
      ],
      "metadata": {
        "id": "v3dG1sz3cnlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ì¢…í•© ì‹¤ìŠµ: ì˜í™” ë¦¬ë·° ë¶„ì„\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ¬ ì¢…í•© ì‹¤ìŠµ: ì˜í™” ë¦¬ë·° ìœ ì‚¬ë„ ë¶„ì„\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "ğŸ’¡ ì‹¤ì „ ì ìš©:\n",
        "ì‹¤ì œ ì˜í™” ë¦¬ë·°ë¥¼ ë²¡í„°ë¡œ ë°”ê¿”ì„œ ë¹„ìŠ·í•œ ë¦¬ë·°ë¥¼ ì°¾ì•„ë´ìš”!\n",
        "\"\"\")\n",
        "\n",
        "# ì˜ˆì œ: ì˜í™” ë¦¬ë·°ë“¤\n",
        "reviews = {\n",
        "    \"Ahn\": \"ì¬ë¯¸ìˆê³  ê°ë™ì ì¸ ì˜í™”\",\n",
        "    \"Kim\": \"ì§€ë£¨í•˜ê³  ì¬ë¯¸ì—†ëŠ” ì˜í™”\",\n",
        "    \"Park\": \"ê°ë™ì ì´ê³  ëˆˆë¬¼ë‚˜ëŠ” ì˜í™”\",\n",
        "    \"Jee\": \"ì•¡ì…˜ì´ ë§ê³  ì¬ë¯¸ìˆëŠ” ì˜í™”\",\n",
        "    \"Seo\": \"ì§€ë£¨í•˜ê³  ì¡¸ë¦° ì˜í™”\"\n",
        "}\n",
        "\n",
        "print(\"\\nì˜í™” ë¦¬ë·°ë“¤:\")\n",
        "for name, review in reviews.items():\n",
        "    print(f\"  {name}: {review}\")\n",
        "\n",
        "# ë‹¨ì–´ ì‚¬ì „ ë§Œë“¤ê¸°\n",
        "all_review_words = []\n",
        "for review in reviews.values():\n",
        "    all_review_words.extend(review.split())\n",
        "review_vocab = sorted(set(all_review_words))\n",
        "\n",
        "print(f\"\\në‹¨ì–´ ì‚¬ì „: {review_vocab}\")\n",
        "\n",
        "# ë¦¬ë·°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
        "print(\"\\nê° ë¦¬ë·°ì˜ ë²¡í„°:\")\n",
        "review_vectors = {}\n",
        "for name, review in reviews.items():\n",
        "    words = review.split()\n",
        "    vector = [words.count(word) for word in review_vocab]\n",
        "    review_vectors[name] = vector\n",
        "    print(f\"  {name}: {vector}\")\n",
        "\n",
        "# ë¦¬ë·°Aì™€ ë‹¤ë¥¸ ë¦¬ë·°ë“¤ì˜ ìœ ì‚¬ë„\n",
        "print(\"\\nì•ˆì™€ ë‹¤ë¥¸ ë¦¬ë·°ë“¤ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„:\")\n",
        "base_review = \"Ahn\"\n",
        "similarities = {}\n",
        "\n",
        "for name, vector in review_vectors.items():\n",
        "    if name != base_review:\n",
        "        sim = cosine_sim(review_vectors[base_review], vector)\n",
        "        similarities[name] = sim\n",
        "        print(f\"  {base_review} â†” {name}: {sim:.3f}\")\n",
        "\n",
        "# ê°€ì¥ ë¹„ìŠ·í•œ ë¦¬ë·° ì°¾ê¸°\n",
        "most_similar_review = max(similarities, key=similarities.get)\n",
        "least_similar_review = min(similarities, key=similarities.get)\n",
        "\n",
        "print(f\"\\nâœ… '{reviews[base_review]}'ì™€ ê°€ì¥ ë¹„ìŠ·í•œ ë¦¬ë·°:\")\n",
        "print(f\"   {most_similar_review}: '{reviews[most_similar_review]}' (ìœ ì‚¬ë„: {similarities[most_similar_review]:.3f})\")\n",
        "\n",
        "print(f\"\\nâŒ '{reviews[base_review]}'ì™€ ê°€ì¥ ë‹¤ë¥¸ ë¦¬ë·°:\")\n",
        "print(f\"   {least_similar_review}: '{reviews[least_similar_review]}' (ìœ ì‚¬ë„: {similarities[least_similar_review]:.3f})\")\n",
        "\n",
        "# ìœ ì‚¬ë„ í–‰ë ¬ ì‹œê°í™”\n",
        "print(\"\\n[ì‹œê°í™”] ë¦¬ë·° ê°„ ìœ ì‚¬ë„ íˆíŠ¸ë§µ\")\n",
        "\n",
        "review_names = list(reviews.keys())\n",
        "n = len(review_names)\n",
        "similarity_matrix = np.zeros((n, n))\n",
        "\n",
        "for i, name1 in enumerate(review_names):\n",
        "    for j, name2 in enumerate(review_names):\n",
        "        if i == j:\n",
        "            similarity_matrix[i][j] = 1.0\n",
        "        else:\n",
        "            similarity_matrix[i][j] = cosine_sim(review_vectors[name1], review_vectors[name2])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto')\n",
        "plt.colorbar(label='Cosine Similarity')\n",
        "plt.xticks(range(n), review_names)\n",
        "plt.yticks(range(n), review_names)\n",
        "plt.title('Movie Review Similarity Heatmap', fontsize=14, fontweight='bold')\n",
        "\n",
        "# ê° ì…€ì— ìœ ì‚¬ë„ ê°’ í‘œì‹œ\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        plt.text(j, i, f'{similarity_matrix[i][j]:.2f}',\n",
        "                ha='center', va='center', color='black' if similarity_matrix[i][j] < 0.5 else 'white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BMM3eiaMc5cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë„ì „ 1: ë‚˜ë§Œì˜ ë‹¨ì–´ ë²¡í„° ë§Œë“¤ê¸°\n",
        "## ì¢‹ì•„í•˜ëŠ” K-POP ê·¸ë£¹ 5ê°œë¥¼ [ë…¸ë˜, ì¶¤] ì ìˆ˜ë¡œ ë²¡í„°í™”í•˜ê³ ,\n",
        "## ê°€ì¥ ë¹„ìŠ·í•œ ë‘ ê·¸ë£¹ì„ ì°¾ì•„ë³´ì„¸ìš”!"
      ],
      "metadata": {
        "id": "3u3mpL1-r-66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ### ë„ì „ 2: ì¹œêµ¬ ë¦¬ë·° ë¶„ì„\n",
        "## ì¹œêµ¬ 10ëª…ì—ê²Œ ì¢‹ì•„í•˜ëŠ” ìŒì‹ì— ëŒ€í•œ í•œ ì¤„ í‰ì„ ë°›ì•„ì„œ,\n",
        "## Bag of Wordsë¡œ ë²¡í„°í™”í•˜ê³  ëˆ„ê°€ ë¹„ìŠ·í•œ ì·¨í–¥ì¸ì§€ ì°¾ì•„ë³´ì„¸ìš”!"
      ],
      "metadata": {
        "id": "xjohRxyHsHIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ğŸ”¥ ë„ì „ 1: ë‚˜ë§Œì˜ ë‹¨ì–´ ë²¡í„° ë§Œë“¤ê¸° - K-POP ê·¸ë£¹ ìœ ì‚¬ë„\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ”¥ ë„ì „ 1: K-POP ê·¸ë£¹ì„ ë²¡í„°ë¡œ í‘œí˜„í•˜ê³  ìœ ì‚¬ë„ ë¹„êµí•˜ê¸°\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "[ì„¤ëª…]\n",
        "ì¢‹ì•„í•˜ëŠ” K-POP ê·¸ë£¹ 5ê°œë¥¼ ê³¨ë¼ì„œ\n",
        "[ë…¸ë˜, ì¶¤] ì ìˆ˜(1~10)ë¥¼ ì£¼ê³  ë²¡í„°ë¡œ í‘œí˜„í•œ ë’¤,\n",
        "ê°€ì¥ ë¹„ìŠ·í•œ ë‘ ê·¸ë£¹ì„ ì°¾ì•„ë´…ë‹ˆë‹¤!\n",
        "\n",
        "â€» ì•„ë˜ ì˜ˆì‹œëŠ” ì„ì˜ë¡œ ë„£ì€ ê°’ì…ë‹ˆë‹¤.\n",
        "   ì§ì ‘ ì¢‹ì•„í•˜ëŠ” ê·¸ë£¹ìœ¼ë¡œ ë°”ê¿” ë³´ì„¸ìš” :)\n",
        "\"\"\")\n",
        "\n",
        "# K-POP ê·¸ë£¹ ë²¡í„° ì˜ˆì‹œ: [ë…¸ë˜, ì¶¤]\n",
        "kpop_groups = {\n",
        "    \"NewJeans\": [9, 8],\n",
        "    \"IVE\": [8, 7],\n",
        "    \"LE SSERAFIM\": [7, 9],\n",
        "    \"SEVENTEEN\": [8, 9],\n",
        "    \"NCT\": [7, 8]\n",
        "}\n",
        "\n",
        "print(\"\\nK-POP ê·¸ë£¹ ë²¡í„°:\")\n",
        "for name, vec in kpop_groups.items():\n",
        "    print(f\"  {name}: {vec}\")\n",
        "\n",
        "# ëª¨ë“  ê·¸ë£¹ ìŒì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "group_names = list(kpop_groups.keys())\n",
        "best_pair = None\n",
        "best_similarity = -1  # ê°€ì¥ í° ìœ ì‚¬ë„ ì°¾ê¸°\n",
        "\n",
        "print(\"\\n[ì‹¤ìŠµ] ê·¸ë£¹ ìŒë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„:\")\n",
        "for i in range(len(group_names)):\n",
        "    for j in range(i + 1, len(group_names)):\n",
        "        g1 = group_names[i]\n",
        "        g2 = group_names[j]\n",
        "        v1 = kpop_groups[g1]\n",
        "        v2 = kpop_groups[g2]\n",
        "        sim = cosine_sim(v1, v2)\n",
        "        print(f\"  {g1} â†” {g2}: {sim:.3f}\")\n",
        "        if sim > best_similarity:\n",
        "            best_similarity = sim\n",
        "            best_pair = (g1, g2)\n",
        "\n",
        "print(\"\\nâœ… ê°€ì¥ ë¹„ìŠ·í•œ ë‘ ê·¸ë£¹:\")\n",
        "print(f\"  {best_pair[0]} â†” {best_pair[1]} (ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {best_similarity:.3f})\")\n",
        "\n",
        "# ì„ íƒ: 2ì°¨ì› í‰ë©´ì— ì‹œê°í™”\n",
        "plt.figure(figsize=(8, 6))\n",
        "for name, vec in kpop_groups.items():\n",
        "    x, y = vec\n",
        "    plt.scatter(x, y, s=200, alpha=0.7)\n",
        "    plt.annotate(name, (x, y), fontsize=12, ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel(\"ë…¸ë˜ (Vocal / Song)\", fontsize=12)\n",
        "plt.ylabel(\"ì¶¤ (Dance / Performance)\", fontsize=12)\n",
        "plt.title(\"K-POP ê·¸ë£¹ [ë…¸ë˜, ì¶¤] ë²¡í„° ì‹œê°í™”\", fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(0, 11)\n",
        "plt.ylim(0, 11)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0RQOlD3AsLJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ğŸ”¥ ë„ì „ 2: ì¹œêµ¬ ìŒì‹ ë¦¬ë·° ë¶„ì„ - Bag of Words + ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ”¥ ë„ì „ 2: ì¹œêµ¬ 10ëª…ì˜ ìŒì‹ ë¦¬ë·° ì·¨í–¥ ë¶„ì„\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "[ì„¤ëª…]\n",
        "ì¹œêµ¬ 10ëª…ì—ê²Œ 'ì¢‹ì•„í•˜ëŠ” ìŒì‹'ì— ëŒ€í•œ í•œ ì¤„ ë¦¬ë·°ë¥¼ ë°›ì•˜ë‹¤ê³  ê°€ì •í•˜ê³ ,\n",
        "Bag of Words ë°©ì‹ìœ¼ë¡œ ë²¡í„°í™”í•œ ë’¤,\n",
        "ì„œë¡œ ê°€ì¥ ì·¨í–¥ì´ ë¹„ìŠ·í•œ ì¹œêµ¬ ìŒì„ ì°¾ì•„ë´…ë‹ˆë‹¤.\n",
        "\n",
        "â€» ì‹¤ì œë¡œëŠ” ì¹œêµ¬ ë¦¬ë·°ë¥¼ ì§ì ‘ ë°›ì•„ì„œ ì•„ë˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.\n",
        "\"\"\")\n",
        "\n",
        "friends_reviews = {\n",
        "    \"Friend1\": \"ë§¤ìš´ ìŒì‹ì´ ë„ˆë¬´ ì¢‹ì•„ìš”\",\n",
        "    \"Friend2\": \"ë‹¬ì½¤í•œ ë””ì €íŠ¸ë¥¼ ì •ë§ ì¢‹ì•„í•´ìš”\",\n",
        "    \"Friend3\": \"ë‹´ë°±í•œ í•œì‹ì„ ì œì¼ ì¢‹ì•„í•©ë‹ˆë‹¤\",\n",
        "    \"Friend4\": \"ê¸°ë¦„ì§„ íŒ¨ìŠ¤íŠ¸í‘¸ë“œë¥¼ ìì£¼ ë¨¹ì–´ìš”\",\n",
        "    \"Friend5\": \"í•´ì‚°ë¬¼ ìš”ë¦¬ë¥¼ íŠ¹íˆ ì¢‹ì•„í•´ìš”\",\n",
        "    \"Friend6\": \"ë§¤ìš´ ì¹˜í‚¨ê³¼ ë–¡ë³¶ì´ë¥¼ ì‚¬ë‘í•©ë‹ˆë‹¤\",\n",
        "    \"Friend7\": \"ìƒëŸ¬ë“œ ê°™ì€ ê°€ë²¼ìš´ ìŒì‹ì„ ì¢‹ì•„í•´ìš”\",\n",
        "    \"Friend8\": \"ë‹¬ì½¤í•œ ì¼€ì´í¬ì™€ ë¹µì„ ìì£¼ ë¨¹ì–´ìš”\",\n",
        "    \"Friend9\": \"ëœ¨ëˆí•œ êµ­ë¬¼ ìš”ë¦¬ë¥¼ ì¢‹ì•„í•´ìš”\",\n",
        "    \"Friend10\": \"ì¹˜ì¦ˆ ë“¬ë¿ ë“¤ì–´ê°„ í”¼ìë¥¼ ì œì¼ ì¢‹ì•„í•´ìš”\"\n",
        "}\n",
        "\n",
        "print(\"\\nì¹œêµ¬ë“¤ì˜ ë¦¬ë·°:\")\n",
        "for name, review in friends_reviews.items():\n",
        "    print(f\"  {name}: {review}\")\n",
        "\n",
        "# 1. ë‹¨ì–´ ì‚¬ì „(vocabulary) ë§Œë“¤ê¸°\n",
        "all_food_words = []\n",
        "for review in friends_reviews.values():\n",
        "    all_food_words.extend(review.split())\n",
        "\n",
        "vocab = sorted(set(all_food_words))\n",
        "print(f\"\\në‹¨ì–´ ì‚¬ì „ (vocabulary): {vocab}\")\n",
        "print(f\"(ì´ {len(vocab)}ê°œ ë‹¨ì–´)\\n\")\n",
        "\n",
        "# 2. ê° ë¦¬ë·°ë¥¼ Bag of Words ë²¡í„°ë¡œ ë³€í™˜\n",
        "friend_vectors = {}\n",
        "\n",
        "print(\"[ì‹¤ìŠµ] ê° ì¹œêµ¬ ë¦¬ë·°ì˜ ë²¡í„° í‘œí˜„:\")\n",
        "for name, review in friends_reviews.items():\n",
        "    words = review.split()\n",
        "    vector = [words.count(word) for word in vocab]\n",
        "    friend_vectors[name] = vector\n",
        "\n",
        "    print(f\"\\n{name}: '{review}'\")\n",
        "    print(f\"  ë²¡í„°: {vector}\")\n",
        "    print(f\"  ì˜ë¯¸: \", end=\"\")\n",
        "    for i, word in enumerate(vocab):\n",
        "        if vector[i] > 0:\n",
        "            print(f\"{word}({vector[i]}ë²ˆ) \", end=\"\")\n",
        "    print()\n",
        "\n",
        "# 3. ì¹œêµ¬ë“¤ ê°„ ì·¨í–¥ ìœ ì‚¬ë„(ì½”ì‚¬ì¸ ìœ ì‚¬ë„) ê³„ì‚°\n",
        "friend_names = list(friends_reviews.keys())\n",
        "max_similarity = -1\n",
        "most_similar_pair = None\n",
        "\n",
        "print(\"\\n[ì‹¤ìŠµ] ì¹œêµ¬ ìŒë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„:\")\n",
        "for i in range(len(friend_names)):\n",
        "    for j in range(i + 1, len(friend_names)):\n",
        "        f1 = friend_names[i]\n",
        "        f2 = friend_names[j]\n",
        "        v1 = friend_vectors[f1]\n",
        "        v2 = friend_vectors[f2]\n",
        "        sim = cosine_sim(v1, v2)\n",
        "        print(f\"  {f1} â†” {f2}: {sim:.3f}\")\n",
        "        if sim > max_similarity:\n",
        "            max_similarity = sim\n",
        "            most_similar_pair = (f1, f2)\n",
        "\n",
        "print(\"\\nâœ… ìŒì‹ ì·¨í–¥ì´ ê°€ì¥ ë¹„ìŠ·í•œ ì¹œêµ¬ ìŒ:\")\n",
        "print(f\"  {most_similar_pair[0]} â†” {most_similar_pair[1]} (ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {max_similarity:.3f})\")\n",
        "\n",
        "# ì„ íƒ: ìœ ì‚¬ë„ í–‰ë ¬ë¡œ Heatmap ì‹œê°í™” (ì˜µì…˜)\n",
        "n = len(friend_names)\n",
        "similarity_matrix = np.zeros((n, n))\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        v1 = friend_vectors[friend_names[i]]\n",
        "        v2 = friend_vectors[friend_names[j]]\n",
        "        similarity_matrix[i][j] = cosine_sim(v1, v2)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto')\n",
        "plt.colorbar(label='Cosine Similarity')\n",
        "plt.xticks(range(n), friend_names, rotation=45)\n",
        "plt.yticks(range(n), friend_names)\n",
        "plt.title('ì¹œêµ¬ ìŒì‹ ì·¨í–¥ ìœ ì‚¬ë„ Heatmap', fontsize=14, fontweight='bold')\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        plt.text(j, i, f\"{similarity_matrix[i][j]:.2f}\",\n",
        "                 ha='center', va='center',\n",
        "                 color='black' if similarity_matrix[i][j] < 0.5 else 'white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8dEtFEms6Qy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}